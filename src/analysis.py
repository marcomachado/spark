# %%
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, avg, stddev, expr
from pyspark.sql.types import DoubleType 
import pandas

# Cria uma sessão Spark
spark = SparkSession.builder \
    .appName("Record Linkage") \
    .getOrCreate()

spark.sparkContext.setLogLevel('WARN')

# %% read CSV and show DataFrame
#df = spark.read.csv("data/block*.csv")
#print(df.show(5))

# %% inferSchema
parsed = spark.read.option("header", "true").option("nullValue", "?")\
.option("inferSchema", "true").csv("data/block*.csv")

print("\ninferSchema generated by spark")
parsed.printSchema()
print("\n TOP 5 with new schema")
parsed.show(5)
print("\nparsed Total")
print(parsed.count())

# %%
parsed.cache()

# %% Aggregation functions
print("\nTotal is_match true/false")
parsed.groupBy("is_match").count().orderBy(col("count").desc()).show()

print("\ncalculating mean and standard deviation from cmp_sex")
parsed.agg(avg("cmp_sex"), stddev("cmp_sex")).show()

# using SQL
print("\Calculating total 'is_match' true/false usign SQL")
parsed.createOrReplaceTempView("linkage")
spark.sql(""" SELECT is_match, COUNT(*) cnt 
          FROM linkage 
          GROUP BY is_match 
          ORDER BY cnt DESC """).show()


# %% summary statistics
print("\nDescribe parsed DF: summary")
summary = parsed.describe()
summary.show()
print("\nSome columns from Summary")
summary.select("summary", "cmp_fname_c1", "cmp_fname_c2").show()

print("less than 2% of the records have a non-null value for cmp_fname_c2")



# filter sql-style
matches = parsed.where("is_match = true")
match_summary = matches.describe()

#filter Column DataFrame API
misses = parsed.filter(col("is_match") == False)
miss_summary = misses.describe()


# Pivoting and Reshaping DataFrames
summary_p = summary.toPandas()

print("\nPivoting summary DF")
summary_p.head()
print(summary_p.shape)

# transpose
summary_p = summary_p.set_index('summary').transpose().reset_index()
summary_p = summary_p.rename(columns={'index':'field'})
summary_p = summary_p.rename_axis(None, axis=1)

print("\ntransposing summary-P DF\n")
print(summary_p.shape)

summaryT = spark.createDataFrame(summary_p)
summaryT.show()

# all fields are string type
print("\nSummaryT: all fields are string type")
summaryT.printSchema()

for c in summaryT.columns: 
    if c == 'field': 
        continue  
    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))

print("\nSummaryT with columns-number converted")
summaryT.printSchema()

#function to transpose and convert fields to Double
def pivot_summary(desc):  
    desc_p = desc.toPandas()
    desc_p = desc_p.set_index('summary').transpose().reset_index()
    desc_p = desc_p.rename(columns={'index':'field'})
    desc_p = desc_p.rename_axis(None, axis=1) 
    descT = spark.createDataFrame(desc_p)
    for c in descT.columns: 
        if c == 'field': 
            continue 
        else:
            descT = descT.withColumn(c, descT[c].cast(DoubleType())) 
    return descT

print("\nUsing pivot_summary function to pivot, transpose DF and convert number fields")
match_summaryT = pivot_summary(match_summary)
miss_summaryT = pivot_summary(miss_summary)


# joining dataframes
print("\nCreating Views to match_summaryT and miss_summaryT and joining using Spark SQL")
match_summaryT.createOrReplaceTempView("match_desc")
miss_summaryT.createOrReplaceTempView("miss_desc")

print("\nShowing features analysis")
spark.sql("""
SELECT a.field, a.count + b.count total, a.mean - b.mean delta
FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field
WHERE a.field NOT IN ("id_1", "id_2")
ORDER BY delta DESC, total DESC
""").show()
print("Good Features: higher total and delta")
print("delta: difference between means")
print("Selected features:")
print("cmp_plz,  cmp_by, cmp_bd, cmp_lname_c1, and cmp_bm. ")
print("values missing = 0")

# Scoring and Model Evaluation
good_features = ["cmp_lname_c1", "cmp_plz", "cmp_by", "cmp_bd", "cmp_bm"]
sum_expression = " + ".join(good_features)

print("\nSumming features")
scored = parsed.fillna(0, subset=good_features).\
    withColumn('score', expr(sum_expression)).\
    select('score', 'is_match')

scored.show()    

def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame: 
    return scored.selectExpr(f"score >= {t} as above", "is_match").\
        groupBy("above").pivot("is_match", ("true", "false")).\
        count()


print("\nTesting high threshold (limit):4.0 -> filter almost all nonmatches")
print("and keep over 90%  of the matches")
crossTabs(scored, 4.0).show()

print("\nTesting low threshold (limit):2.0 -> high the false-positive rate: 5131787")
crossTabs(scored, 2.0).show()

print("\nTrying to use other values from MatchData (both missing and not") 
print("to identify 100% of TRUE matches with MAX 100 false positives (TOP-RiGHT field)")


# Para encerrar a sessão Spark
spark.stop()